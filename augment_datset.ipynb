{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabbiurlaro/aml22-ego/blob/vae/augment_datset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pW53_TEF0JuB"
      },
      "source": [
        "# INSTALL AND GIT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GdURZvMjb9U",
        "outputId": "e9294266-78fa-4bf9-f247-343c79a63baf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'aml22-ego'...\n",
            "remote: Enumerating objects: 3606, done.\u001b[K\n",
            "remote: Counting objects: 100% (258/258), done.\u001b[K\n",
            "remote: Compressing objects: 100% (129/129), done.\u001b[K\n",
            "remote: Total 3606 (delta 167), reused 206 (delta 128), pack-reused 3348\u001b[K\n",
            "Receiving objects: 100% (3606/3606), 1.51 GiB | 31.44 MiB/s, done.\n",
            "Resolving deltas: 100% (2633/2633), done.\n",
            "Updating files: 100% (45/45), done.\n",
            "Updating files: 100% (161/161), done.\n",
            "Branch 'vae' set up to track remote branch 'vae' from 'origin'.\n",
            "Switched to a new branch 'vae'\n"
          ]
        }
      ],
      "source": [
        "!rm -rf sample_data\n",
        "\n",
        "!git clone https://github.com/gabbiurlaro/aml22-ego.git aml22-ego\n",
        "!cd aml22-ego && git checkout vae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhUB2tNujdxM",
        "outputId": "ce6bb28d-efcc-4799-ef01-45892f5255af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuAI_gnfjeQj",
        "outputId": "ee8caeca-04cc-448b-bd87-a906f275f600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è¨ Downloading https://github.com/conda-forge/miniforge/releases/download/23.1.0-1/Mambaforge-23.1.0-1-Linux-x86_64.sh...\n",
            "üì¶ Installing...\n",
            "üìå Adjusting configuration...\n",
            "ü©π Patching environment...\n",
            "‚è≤ Done in 0:00:12\n",
            "üîÅ Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "# Install conda\n",
        "\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jItTAWYnuUO5"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /usr/local/envs/egovision\n",
        "!tar xf /content/drive/MyDrive/egovision.tar.gz --directory=/usr/local/envs/egovision"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3CHT3O2ka_M5"
      },
      "source": [
        "# Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pVgmYFLjPgt"
      },
      "outputs": [],
      "source": [
        "import pickle \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pywt\n",
        "import torch\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from scipy.interpolate import CubicSpline\n",
        "import random\n",
        "import torchvision.transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "train =  pd.DataFrame(pd.read_pickle('/content/aml22-ego/saved_features/ACTIONNET_EMG/EMG_no-clip_ActionNet_train.pkl'))\n",
        "test =  pd.DataFrame(pd.read_pickle('/content/aml22-ego/saved_features/ACTIONNET_EMG/EMG_no-clip_ActionNet_test.pkl'))\n",
        "original_train = pd.DataFrame(pd.read_pickle('/content/drive/MyDrive/train_val_EMG/ActionNet_train.pkl'))\n",
        "original_test = pd.DataFrame(pd.read_pickle('/content/drive/MyDrive/train_val_EMG/ActionNet_test.pkl'))\n",
        "original_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddtzSq60jPgw"
      },
      "outputs": [],
      "source": [
        "from utils.loaders import ActionNetDataset\n",
        "sigma = 0.1\n",
        "wavelet_name = 'db7' #Wavelet name (e.g., Daubechies 4)\n",
        "decomposition_level = 5 # # Number of decomposition levels\n",
        "detail_factor = 0 # Scaling factor for modifying detail coefficients\n",
        "  \n",
        "num_clips = 5\n",
        "batch_Size = 1\n",
        "\n",
        "\n",
        "train =  ActionNetDataset('ActionNet', ['EMG'], 'train', {'stride': 2, 'annotations_path':'/content/drive/MyDrive/train_val_EMG'}, {'EMG': 32}, 5, {'EMG': False},\n",
        "                                                                       None, load_feat=False, additional_info=True)\n",
        "#pd.DataFrame(pd.read_pickle('/content/aml22-ego/saved_features/ACTIONNET_EMG/EMG_no-clip_ActionNet_train.pkl'))\n",
        "test =  ActionNetDataset('ActionNet', ['EMG'], 'test', {'stride': 2, 'annotations_path':'/content/drive/MyDrive/train_val_EMG'}, {'EMG': 32}, 5, {'EMG': False},\n",
        "                                                                       None, load_feat=False,  additional_info=True)\n",
        "\n",
        "train.list_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "hw5qXKROjPgv"
      },
      "outputs": [],
      "source": [
        "def wavelet_decomposition(signal, wavelet_name, decomposition_level, detail_factor):\n",
        "    #print('WD :', signal.shape)\n",
        "    coeffs = pywt.wavedec(signal, wavelet=wavelet_name, level=decomposition_level)\n",
        "    cA, cD = coeffs[0], coeffs[1:]  # Approximation and detail coefficients\n",
        "    \n",
        "    # Modify detail coefficients\n",
        "    cD_modified = [detail_factor * cd for cd in cD]\n",
        "    \n",
        "    # Reconstruct the augmented signal\n",
        "    augmented_coeffs = [cA] + cD_modified\n",
        "    augmented_signal = torch.tensor(pywt.waverec(augmented_coeffs, wavelet=wavelet_name))\n",
        "    \n",
        "    return augmented_signal\n",
        "\n",
        "\n",
        "class WaveletDecompositionTransform:\n",
        "    def __init__(self, wavelet_name, decomposition_level, detail_factor, num_clips):\n",
        "        self.wavelet_name = wavelet_name\n",
        "        self.decomposition_level = decomposition_level\n",
        "        self.detail_factor = detail_factor\n",
        "        self.num_clips = num_clips\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        signals = sample['features_EMG']\n",
        "        augmented_signals = []\n",
        "        for i in range(self.num_clips):\n",
        "          \n",
        "          augmented_signals.append(wavelet_decomposition(signals[i], self.wavelet_name, self.decomposition_level, self.detail_factor))\n",
        "        # Create a new augmented sample\n",
        "        augmented_sample = {\n",
        "            'features_EMG': torch.stack(augmented_signals).reshape(5, 16, 32, 32),\n",
        "            'label': sample['label'],\n",
        "            'uid': sample['uid'],\n",
        "            'untrimmed_video_name': sample['untrimmed_video_name']\n",
        "        }\n",
        "        #print('WD',augmented_sample['features_EMG'].shape)\n",
        "        \n",
        "        return augmented_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "LGxv8xQ7jPgv"
      },
      "outputs": [],
      "source": [
        "def magnitude_warping(signal, variance=0.01):\n",
        "    T = signal.size(1)\n",
        "    #print('MW :', T)\n",
        "    t = torch.linspace(0, 1, T)  # Equidistant time points\n",
        "    r = torch.randn(T)  # Random numbers from a normal distribution\n",
        "    r = torch.clamp(r, -2.0, 2.0)  # Limit the range of random numbers to avoid extreme warping\n",
        "    \n",
        "    # Generate a smooth curve using cubic splines\n",
        "    spline = CubicSpline(t, r)\n",
        "    cubic_spline = torch.from_numpy(spline(t)).float()\n",
        "    \n",
        "    # Elementwise product of the interpolated curve with the signal\n",
        "    warped_signal = torch.Tensor(signal * (1.0 + variance * cubic_spline))\n",
        "\n",
        "    return warped_signal\n",
        "\n",
        "class MagnitudeWarpingTransform:\n",
        "    def __init__(self, variance, num_clips):\n",
        "        self.variance= variance\n",
        "        self.num_clips = num_clips\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        signals = sample['features_EMG']\n",
        "        augmented_signals = []\n",
        "        for i in range(self.num_clips):\n",
        "         # print('MW1: ', signals.shape)\n",
        "          augmented_signals.append(magnitude_warping(signals[i], variance=self.variance))\n",
        "        # Create a new augmented sample\n",
        "        augmented_sample = {\n",
        "            'features_EMG': torch.stack(augmented_signals),\n",
        "            'label': sample['label'],\n",
        "            'uid': sample['uid'],\n",
        "            'untrimmed_video_name': sample['untrimmed_video_name']\n",
        "        }\n",
        "        \n",
        "        return augmented_sample\n",
        "\n",
        "# Example usage\n",
        "#signal = torch.randn(1024)  # Assuming input signal of size 1024\n",
        "#warped_signal = magnitude_warping(signal, variance=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "mM4iZP3__zJr"
      },
      "outputs": [],
      "source": [
        "class Fit_Dims:\n",
        "    def __init__(self, num_clips=5):\n",
        "        \n",
        "        self.num_clips = num_clips\n",
        "\n",
        "    def __call__(self, x):\n",
        "        signals = x[0]['EMG']\n",
        "        signals = signals.reshape(5,16, 32, 32)\n",
        "        #print(signals.shape)\n",
        "        augmented_sample = {\n",
        "              'features_EMG':signals,\n",
        "              'label': x[1],\n",
        "              'uid': x[3],\n",
        "              'untrimmed_video_name': x[2]\n",
        "          }\n",
        "        #print(augmented_sample['features_EMG'].shape)\n",
        "        return augmented_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH6YLd_lDX8T",
        "outputId": "ba35c050-33f3-4082-c32f-0742104c9171"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fd <class 'torch.Tensor'> torch.Size([160, 32])\n",
            "fd <class 'torch.Tensor'> torch.Size([5, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "signal = train[0][0]['EMG']\n",
        "signal = signal[0]\n",
        "print('fd', type(signal), signal.shape)\n",
        "x = signal.reshape(5,-1,32)\n",
        "print('fd', type(x), x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H0IVcLW1--Z",
        "outputId": "7e11a3cc-7ba2-4bd4-820b-fe614518eba7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mACTIONNET\u001b[0m/  \u001b[01;34mACTIONNET_EMG\u001b[0m/  \u001b[01;34maug\u001b[0m/  \u001b[01;34maug_original\u001b[0m/  \u001b[01;34mEPIC\u001b[0m/  \u001b[01;34mreconstructed\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls\n",
        "for i in range(4):\n",
        "  filename = './aug_original/ActionNet_augmented_clips_' + ts[i]\n",
        "  with open(f\"{filename}_train.pkl\", \"wb\") as file:\n",
        "            pickle.dump(outs[0][i], file)\n",
        "  with open(f\"{filename}_test.pkl\", \"wb\") as file:\n",
        "             pickle.dump(outs[1][i], file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EODm1GEw1Y_-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "mSjZDYKLBh_h",
        "outputId": "76b40f76-4922-46ad-b301-d916b4d08554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/aml22-ego/saved_features/ACTIONNET_EMG_AUG/ (stored 0%)\n",
            "  adding: content/aml22-ego/saved_features/ACTIONNET_EMG_AUG/job_feature_extraction_test_train.pkl (deflated 11%)\n",
            "  adding: content/aml22-ego/saved_features/ACTIONNET_EMG_AUG/job_feature_extraction_train_train.pkl (deflated 11%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_839083ef-ae2d-402e-a1a8-620d024b1724\", \"feats_augs.zip\", 2209072)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "!zip -r /content/feats_augs.zip /content/aml22-ego/saved_features/ACTIONNET_EMG_AUG\n",
        "files.download('/content/feats_augs.zip')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UR-pPX-qbKtf"
      },
      "source": [
        "# Plot features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4Xev75IbKQG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZevU9cpo99zV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "num_clips = 5\n",
        "# EPIC-KITCHEN and ActionNet\n",
        "\n",
        "labels = {'EK':{\n",
        "        0 : \"take (get)\",\n",
        "        1 : \"put-down (put/place)\",\n",
        "        2 : \"open\",\n",
        "        3 : \"close\",\n",
        "        4 : \"wash (clean)\",\n",
        "        5 : \"cut\",\n",
        "        6 : \"stir (mix)\",\n",
        "        7 : \"pour\"\n",
        "}, 'AN': {\n",
        "        0 : \"Spread\",\n",
        "        1 : \"Get/Put\",\n",
        "        2 : \"Clear\",\n",
        "        3 : \"Slice\",\n",
        "        4 : \"Clean\",\n",
        "        5 : \"Pour\",\n",
        "        6 : \"Load\",\n",
        "        7 : \"Peel\",\n",
        "        8 : \"Open/Close\",\n",
        "        9 : \"Set\",\n",
        "        10 : \"Stack\",\n",
        "        11 : \"Unload\"\n",
        "\n",
        "}}\n",
        "\n",
        "colors = {'EK': {\n",
        "        0 : \"#A52A2A\",\n",
        "        1 : \"#DAA520\",\n",
        "        2 : \"#FF7F50\",\n",
        "        3 : \"#7BC8F6\",\n",
        "        4 : \"#FFFF14\",\n",
        "        5 : \"#76FF7B\",\n",
        "        6 : \"#13EAC9\",\n",
        "        7 : \"olive\"\n",
        "}, 'AN': {\n",
        "        0 : \"#A52A2A\",\n",
        "        1 : \"#DAA520\",\n",
        "        2 : \"#FF7F50\",\n",
        "        3 : \"#7BC8F6\",\n",
        "        4 : \"#FFFF14\",\n",
        "        5 : \"#76FF7B\",\n",
        "        6 : \"#13EAC9\",\n",
        "        7 : \"olive\",\n",
        "        8 : \"palegreen\",\n",
        "        9 : \"lightpink\",\n",
        "        10 : \"darkmagenta\",\n",
        "        11 : \"cadetblue\"\n",
        "}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i9t4C1m-EJi"
      },
      "outputs": [],
      "source": [
        "data_original = pd.DataFrame(pd.read_pickle('/content/aml22-ego/saved_features/ACTIONNET_EMG/EMG_no-clip_ActionNet_train.pkl')[\"features\"])\n",
        "data = pd.DataFrame(pd.read_pickle(\"/content/aml22-ego/saved_features/reconstructed/AUG_VAE_0.001_2023-05-24 16:15:44.696068_train.pkl\")[\"features\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olEjp7jS_0jQ"
      },
      "outputs": [],
      "source": [
        "data.iloc[0]\n",
        "ts = ['WD-MW', 'MW', 'WD', 'MW-WD']\n",
        "data_augmented_train = []\n",
        "for i in range(4):\n",
        "  data_augmented_train.append(pd.read_pickle('/content/aml22-ego/saved_features/aug/ActionNet_augmented_clips_'+ ts[i]+'_train.pkl'))\n",
        "data = pd.DataFrame(data_augmented_train[3]['features'] +data_augmented_train[2]['features']+\n",
        "                   data_augmented_train[1]['features'] +\n",
        "                    data_augmented_train[0]['features'] + data_original['features'])\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FWl8lnv-3PQ"
      },
      "outputs": [],
      "source": [
        "# plot emg features\n",
        "print(len(data.iloc[0]['features_EMG']))\n",
        "features = np.array([data.iloc[i].features_EMG[num_clips//2] for i in range(len(data))])\n",
        "reduced = TSNE().fit_transform(features)\n",
        "data['x'] = reduced[:, 0]\n",
        "data['y'] = reduced[:, 1]\n",
        "for i in range(12): # ek has 8 classes\n",
        "    filtered = data[data[\"label\"] == i]\n",
        "    # compute the central frame\n",
        "    plt.scatter(filtered['x'], filtered['y'], c=colors['AN'][i], label=labels['AN'][i])\n",
        "plt.legend()\n",
        "plt.title('EMG train features')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mxOr7-o7bQXP"
      },
      "source": [
        "# Train VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHA--nTubPuf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJwhsYyR_oD-"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "#VAE EMG train and save\n",
        "\n",
        "cd aml22-ego && git pull origin vae\n",
        "\n",
        "PYTHON_PATH=/usr/local/envs/egovision/bin/python\n",
        "\n",
        "$PYTHON_PATH train_VAE_features_EMG.py action=\"train_and_save\" \\\n",
        "  name=\"VAE_EMG_2 full-aug lr1e-3 wkld1 sum\" \\\n",
        "  config=configs/VAE_save_feat_EMG.yaml \\\n",
        "  dataset.shift=ActionNet-ActionNet \\\n",
        "  wandb_name='vae' \\\n",
        "  wandb_dir='Experiment_logs'  \\\n",
        "  dataset.RGB.data_path=../ek_data/frames \\\n",
        "  dataset.EMG.features_name='ACTIONNET_EMG/EMG_no-clip' \\\n",
        "  models.EMG.model='VAE' \\\n",
        "  models.EMG.lr=1e-3"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lz0363gSYozR"
      },
      "source": [
        "# Train Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1avbQl8dKAq",
        "outputId": "40a8a207-b7fb-428b-d2ef-9208fdc5540c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aml22-ego\n",
            "condacolab_install.log\n",
            "drive\n",
            "feats_augs.zip\n",
            "feats_def.zip\n",
            "No local changes to save\n",
            "Updating 85e3809..1d1dfd3\n",
            "Fast-forward\n",
            " configs/classifier_emg.yaml                        |   1 +\n",
            " saved_features/.DS_Store                           | Bin 6148 -> 6148 bytes\n",
            " .../job_feature_extraction_test_train.pkl          | Bin 0 -> 249144 bytes\n",
            " .../job_feature_extraction_train_train.pkl         | Bin 0 -> 2227571 bytes\n",
            " train_classifier_EMG.py                            |  66 ++++++++++++++++-----\n",
            " utils/loaders.py                                   |   4 +-\n",
            " 6 files changed, 53 insertions(+), 18 deletions(-)\n",
            " create mode 100644 saved_features/ACTIONNET_EMG_AUG/job_feature_extraction_test_train.pkl\n",
            " create mode 100644 saved_features/ACTIONNET_EMG_AUG/job_feature_extraction_train_train.pkl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "From https://github.com/gabbiurlaro/aml22-ego\n",
            " * branch            vae        -> FETCH_HEAD\n",
            "   8a8750c..1d1dfd3  vae        -> origin/vae\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "ls\n",
        "cd aml22-ego/\n",
        "git stash\n",
        "git pull origin vae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w804gCY2YoE8",
        "outputId": "b2cc78bf-c45e-43dd-b8e5-ce6102b54d51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "bash: line 5: /usr/local/envs/egovision/bin/python: No such file or directory\n"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command 'b'#classifier EMG train, validate and save\\n\\nPYTHON_PATH=/usr/local/envs/egovision/bin/python\\n\\n$PYTHON_PATH train_classifier_EMG.py action=\"job_feature_extraction\" name=\"job_feature_extraction\" \\\\\\n  config=\\'configs/classifier_emg.yaml\\' \\\\\\n  dataset.shift=\\'ActionNet-ActionNet\\' \\\\\\n  train.num_iter=600\\\\\\n  wandb_name=\\'vae\\'\\\\\\n  wandb_dir=\\'Experiment_logs\\'\\\\\\n  dataset.RGB.data_path=\\'../ek_data/frames\\'  \\\\\\n  models.EMG.model=\\'EMG_classifier\\' \\\\\\n  resume_from=\\'./saved_models/EMG_classifier\\' \\\\\\n  dataset.EMG.features_name=\\'ACTIONNET_EMG/EMG_Normalized_no-clip\\' \\n'' returned non-zero exit status 127.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_cell_magic(\u001b[39m'\u001b[39;49m\u001b[39mbash\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m#classifier EMG train, validate and save\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mPYTHON_PATH=/usr/local/envs/egovision/bin/python\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m$PYTHON_PATH train_classifier_EMG.py action=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mjob_feature_extraction\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m name=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mjob_feature_extraction\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m  config=\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mconfigs/classifier_emg.yaml\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m  dataset.shift=\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mActionNet-ActionNet\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m  train.num_iter=600\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m  wandb_name=\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mvae\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m  wandb_dir=\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mExperiment_logs\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m  dataset.RGB.data_path=\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m../ek_data/frames\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m  \u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m  models.EMG.model=\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mEMG_classifier\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m  resume_from=\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m./saved_models/EMG_classifier\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m  dataset.EMG.features_name=\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mACTIONNET_EMG/EMG_Normalized_no-clip\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/IPython/core/interactiveshell.py:2417\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2415\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2416\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2417\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2418\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/IPython/core/magics/script.py:153\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     line \u001b[39m=\u001b[39m script\n\u001b[0;32m--> 153\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshebang(line, cell)\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/IPython/core/magics/script.py:305\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mraise_error \u001b[39mand\u001b[39;00m p\u001b[39m.\u001b[39mreturncode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[39m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[39m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     rc \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mreturncode \u001b[39mor\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m9\u001b[39m\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'#classifier EMG train, validate and save\\n\\nPYTHON_PATH=/usr/local/envs/egovision/bin/python\\n\\n$PYTHON_PATH train_classifier_EMG.py action=\"job_feature_extraction\" name=\"job_feature_extraction\" \\\\\\n  config=\\'configs/classifier_emg.yaml\\' \\\\\\n  dataset.shift=\\'ActionNet-ActionNet\\' \\\\\\n  train.num_iter=600\\\\\\n  wandb_name=\\'vae\\'\\\\\\n  wandb_dir=\\'Experiment_logs\\'\\\\\\n  dataset.RGB.data_path=\\'../ek_data/frames\\'  \\\\\\n  models.EMG.model=\\'EMG_classifier\\' \\\\\\n  resume_from=\\'./saved_models/EMG_classifier\\' \\\\\\n  dataset.EMG.features_name=\\'ACTIONNET_EMG/EMG_Normalized_no-clip\\' \\n'' returned non-zero exit status 127."
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "#classifier EMG train, validate and save\n",
        "\n",
        "PYTHON_PATH=/usr/local/envs/egovision/bin/python\n",
        "\n",
        "$PYTHON_PATH train_classifier_EMG.py action=\"job_feature_extraction\" name=\"job_feature_extraction\" \\\n",
        "  config='configs/classifier_emg.yaml' \\\n",
        "  dataset.shift='ActionNet-ActionNet' \\\n",
        "  train.num_iter=600\\\n",
        "  wandb_name='vae'\\\n",
        "  wandb_dir='Experiment_logs'\\\n",
        "  dataset.RGB.data_path='../ek_data/frames'  \\\n",
        "  models.EMG.model='EMG_classifier' \\\n",
        "  resume_from='./saved_models/EMG_classifier' \\\n",
        "  dataset.EMG.features_name='ACTIONNET_EMG/EMG_Normalized_no-clip' "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
