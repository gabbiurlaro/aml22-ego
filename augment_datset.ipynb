{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# INSTALL AND GIT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GdURZvMjb9U",
        "outputId": "366966fc-c217-4307-a143-afe6b56e3d7a"
      },
      "outputs": [],
      "source": [
        "!rm -rf sample_data\n",
        "\n",
        "!git clone https://github.com/gabbiurlaro/aml22-ego.git aml22-ego\n",
        "!cd aml22-ego && git checkout vae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhUB2tNujdxM",
        "outputId": "66ab0302-43f5-4182-d1bd-fac88e95ed46"
      },
      "outputs": [],
      "source": [
        "# Mount google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuAI_gnfjeQj",
        "outputId": "5ba1ae90-c973-43f5-b749-2df6d94acc64"
      },
      "outputs": [],
      "source": [
        "# Install conda\n",
        "\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jItTAWYnuUO5"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /usr/local/envs/egovision\n",
        "!tar xf /content/drive/MyDrive/egovision.tar.gz --directory=/usr/local/envs/egovision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpFJEDzkjg5d"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cd aml22-ego & git pull origin vae"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3CHT3O2ka_M5"
      },
      "source": [
        "# Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pVgmYFLjPgt"
      },
      "outputs": [],
      "source": [
        "import pickle \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pywt\n",
        "import torch\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from scipy.interpolate import CubicSpline\n",
        "import random\n",
        "import torchvision.transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "train =  pd.DataFrame(pd.read_pickle('/content/aml22-ego/saved_features/ACTIONNET_EMG/EMG_no-clip_ActionNet_train.pkl'))\n",
        "test =  pd.DataFrame(pd.read_pickle('/content/aml22-ego/saved_features/ACTIONNET_EMG/EMG_no-clip_ActionNet_test.pkl'))\n",
        "original_train = pd.DataFrame(pd.read_pickle('/content/drive/MyDrive/train_val_EMG/ActionNet_train.pkl'))\n",
        "original_test = pd.DataFrame(pd.read_pickle('/content/drive/MyDrive/train_val_EMG/ActionNet_test.pkl'))\n",
        "original_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw5qXKROjPgv"
      },
      "outputs": [],
      "source": [
        "def wavelet_decomposition(signal, wavelet_name, decomposition_level, detail_factor):\n",
        "    coeffs = pywt.wavedec(signal, wavelet=wavelet_name, level=decomposition_level)\n",
        "    cA, cD = coeffs[0], coeffs[1:]  # Approximation and detail coefficients\n",
        "    \n",
        "    # Modify detail coefficients\n",
        "    cD_modified = [detail_factor * cd for cd in cD]\n",
        "    \n",
        "    # Reconstruct the augmented signal\n",
        "    augmented_coeffs = [cA] + cD_modified\n",
        "    augmented_signal = np.array(pywt.waverec(augmented_coeffs, wavelet=wavelet_name))\n",
        "    \n",
        "    return augmented_signal\n",
        "\n",
        "\n",
        "class WaveletDecompositionTransform:\n",
        "    def __init__(self, wavelet_name, decomposition_level, detail_factor, num_clips):\n",
        "        self.wavelet_name = wavelet_name\n",
        "        self.decomposition_level = decomposition_level\n",
        "        self.detail_factor = detail_factor\n",
        "        self.num_clips = num_clips\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        signals = sample['features_EMG']\n",
        "        augmented_signals = []\n",
        "        for i in range(self.num_clips):\n",
        "          augmented_signals.append(wavelet_decomposition(torch.Tensor(signals[i]), self.wavelet_name, self.decomposition_level, self.detail_factor))\n",
        "        # Create a new augmented sample\n",
        "        augmented_sample = {\n",
        "            'features_EMG': np.array(augmented_signals),\n",
        "            'label': sample['label'],\n",
        "            'uid': sample['uid'],\n",
        "            'untrimmed_video_name': sample['untrimmed_video_name']\n",
        "        }\n",
        "        \n",
        "        return augmented_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGxv8xQ7jPgv"
      },
      "outputs": [],
      "source": [
        "def magnitude_warping(signal, variance=0.01):\n",
        "    T = signal.size(0)\n",
        "    t = torch.linspace(0, 1, T)  # Equidistant time points\n",
        "    r = torch.randn(T)  # Random numbers from a normal distribution\n",
        "    r = torch.clamp(r, -2.0, 2.0)  # Limit the range of random numbers to avoid extreme warping\n",
        "    \n",
        "    # Generate a smooth curve using cubic splines\n",
        "    spline = CubicSpline(t, r)\n",
        "    cubic_spline = torch.from_numpy(spline(t)).float()\n",
        "    \n",
        "    # Elementwise product of the interpolated curve with the signal\n",
        "    warped_signal = np.array(signal * (1.0 + variance * cubic_spline))\n",
        "\n",
        "    return warped_signal\n",
        "\n",
        "class MagnitudeWarpingTransform:\n",
        "    def __init__(self, variance, num_clips):\n",
        "        self.variance= variance\n",
        "        self.num_clips = num_clips\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        signals = sample['features_EMG']\n",
        "        augmented_signals = []\n",
        "        for i in range(self.num_clips):\n",
        "          augmented_signals.append(magnitude_warping(torch.Tensor(signals[i]), variance=self.variance))\n",
        "        # Create a new augmented sample\n",
        "        augmented_sample = {\n",
        "            'features_EMG': np.array(augmented_signals),\n",
        "            'label': sample['label'],\n",
        "            'uid': sample['uid'],\n",
        "            'untrimmed_video_name': sample['untrimmed_video_name']\n",
        "        }\n",
        "        \n",
        "        return augmented_sample\n",
        "\n",
        "# Example usage\n",
        "#signal = torch.randn(1024)  # Assuming input signal of size 1024\n",
        "#warped_signal = magnitude_warping(signal, variance=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73FRNsZBjPgw"
      },
      "outputs": [],
      "source": [
        "class SlidingWindowTransform:\n",
        "    def __init__(self, window_length, overlap=False, num_clips=5):\n",
        "        self.window_length = window_length\n",
        "        self.overlap = overlap\n",
        "        self.num_clips = num_clips\n",
        "\n",
        "    def __call__(self, x):\n",
        "        signals = x['features_EMG']\n",
        "        for i in range(self.num_clips):\n",
        "          num_segments = len(signals) // self.window_length\n",
        "          print( len(signals))\n",
        "          if self.overlap:\n",
        "              stride = self.window_length // 2\n",
        "          else:\n",
        "              stride = self.window_length\n",
        "          augmented_signals = []\n",
        "          for i in range(num_segments):\n",
        "              start = i * stride\n",
        "              end = start + self.window_length\n",
        "              segment = signals[start:end]\n",
        "              augmented_signals.append(segment)\n",
        "        augmented_sample = {\n",
        "              'features_EMG': torch.stack(augmented_signals),\n",
        "              'label': x['label'],\n",
        "              'uid': x['uid'],\n",
        "              'untrimmed_video_name': x['untrimmed_video_name']\n",
        "          }\n",
        "        \n",
        "        return augmented_sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddtzSq60jPgw",
        "outputId": "fff13fc5-8505-4bf4-9ac9-44d8615f1154"
      },
      "outputs": [],
      "source": [
        "%cd /content/aml22-ego\n",
        "sigma = 0.1\n",
        "wavelet_name = 'db7' #Wavelet name (e.g., Daubechies 4)\n",
        "decomposition_level = 5 # # Number of decomposition levels\n",
        "detail_factor = 0 # Scaling factor for modifying detail coefficients\n",
        "  \n",
        "num_clips = 5\n",
        "\n",
        "train =  pd.DataFrame(pd.read_pickle('/content/aml22-ego/saved_features/ACTIONNET_EMG/EMG_no-clip_ActionNet_train.pkl'))\n",
        "test =  pd.DataFrame(pd.read_pickle('/content/aml22-ego/saved_features/ACTIONNET_EMG/EMG_no-clip_ActionNet_test.pkl'))\n",
        "\n",
        "\n",
        "wavelet_transform = WaveletDecompositionTransform(wavelet_name, decomposition_level, detail_factor, num_clips)\n",
        "magnitude_warp_transform = MagnitudeWarpingTransform(sigma, num_clips)\n",
        "#sw_transform = SlidingWindowTransform(window_length=100, overlap=True, num_clips=num_clips)\n",
        "\n",
        "transforms_A = [\n",
        "    torchvision.transforms.Compose([wavelet_transform, magnitude_warp_transform]),\n",
        "     torchvision.transforms.Compose([magnitude_warp_transform ]), \n",
        "     torchvision.transforms.Compose([wavelet_transform]),\n",
        "     torchvision.transforms.Compose([magnitude_warp_transform, wavelet_transform])\n",
        "    ] \n",
        "out_train = {}\n",
        "out_test = {}\n",
        "outs = [out_train, out_test]\n",
        "datasets = [train, test]\n",
        "\n",
        "for data_id in range(len(datasets)):\n",
        "  for aug_set in range(4):\n",
        "    augmented_samples = [transforms_A[aug_set](datasets[data_id]['features'][i]) for i in range(len(datasets[data_id]['features']))]\n",
        "    outs[data_id][aug_set] = {'features': list(augmented_samples)}\n",
        "    print(len(outs[data_id][aug_set]['features']))\n",
        "  #for feats in datasets[data_id]['features']:\n",
        "\n",
        "ts = ['WD-MW', 'MW', 'WD', 'MW-WD']\n",
        "for i in range(4):\n",
        "  filename = './saved_features/aug/ActionNet_augmented_clips_' + ts[i]\n",
        "  with open(f\"{filename}_train.pkl\", \"wb\") as file:\n",
        "            pickle.dump(outs[0][i], file)\n",
        "  with open(f\"{filename}_test.pkl\", \"wb\") as file:\n",
        "             pickle.dump(outs[1][i], file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "mSjZDYKLBh_h",
        "outputId": "600bf120-a7fe-412c-daa9-864c82206019"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "!zip -r /content/feats_def.zip /content/aml22-ego/saved_features/aug\n",
        "files.download('/content/feats_def.zip')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UR-pPX-qbKtf"
      },
      "source": [
        "# Plot features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4Xev75IbKQG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZevU9cpo99zV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "num_clips = 5\n",
        "# EPIC-KITCHEN and ActionNet\n",
        "\n",
        "labels = {'EK':{\n",
        "        0 : \"take (get)\",\n",
        "        1 : \"put-down (put/place)\",\n",
        "        2 : \"open\",\n",
        "        3 : \"close\",\n",
        "        4 : \"wash (clean)\",\n",
        "        5 : \"cut\",\n",
        "        6 : \"stir (mix)\",\n",
        "        7 : \"pour\"\n",
        "}, 'AN': {\n",
        "        0 : \"Spread\",\n",
        "        1 : \"Get/Put\",\n",
        "        2 : \"Clear\",\n",
        "        3 : \"Slice\",\n",
        "        4 : \"Clean\",\n",
        "        5 : \"Pour\",\n",
        "        6 : \"Load\",\n",
        "        7 : \"Peel\",\n",
        "        8 : \"Open/Close\",\n",
        "        9 : \"Set\",\n",
        "        10 : \"Stack\",\n",
        "        11 : \"Unload\"\n",
        "\n",
        "}}\n",
        "\n",
        "colors = {'EK': {\n",
        "        0 : \"#A52A2A\",\n",
        "        1 : \"#DAA520\",\n",
        "        2 : \"#FF7F50\",\n",
        "        3 : \"#7BC8F6\",\n",
        "        4 : \"#FFFF14\",\n",
        "        5 : \"#76FF7B\",\n",
        "        6 : \"#13EAC9\",\n",
        "        7 : \"olive\"\n",
        "}, 'AN': {\n",
        "        0 : \"#A52A2A\",\n",
        "        1 : \"#DAA520\",\n",
        "        2 : \"#FF7F50\",\n",
        "        3 : \"#7BC8F6\",\n",
        "        4 : \"#FFFF14\",\n",
        "        5 : \"#76FF7B\",\n",
        "        6 : \"#13EAC9\",\n",
        "        7 : \"olive\",\n",
        "        8 : \"palegreen\",\n",
        "        9 : \"lightpink\",\n",
        "        10 : \"darkmagenta\",\n",
        "        11 : \"cadetblue\"\n",
        "}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i9t4C1m-EJi"
      },
      "outputs": [],
      "source": [
        "data_original = pd.DataFrame(pd.read_pickle('/content/aml22-ego/saved_features/ACTIONNET_EMG/EMG_no-clip_ActionNet_train.pkl')[\"features\"])\n",
        "data = pd.DataFrame(pd.read_pickle(\"/content/aml22-ego/saved_features/reconstructed/AUG_VAE_0.001_2023-05-24 16:15:44.696068_train.pkl\")[\"features\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olEjp7jS_0jQ"
      },
      "outputs": [],
      "source": [
        "data.iloc[0]\n",
        "ts = ['WD-MW', 'MW', 'WD', 'MW-WD']\n",
        "data_augmented_train = []\n",
        "for i in range(4):\n",
        "  data_augmented_train.append(pd.read_pickle('/content/aml22-ego/saved_features/aug/ActionNet_augmented_clips_'+ ts[i]+'_train.pkl'))\n",
        "data = pd.DataFrame(data_augmented_train[3]['features'] +data_augmented_train[2]['features']+\n",
        "                   data_augmented_train[1]['features'] +\n",
        "                    data_augmented_train[0]['features'] + data_original['features'])\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "_FWl8lnv-3PQ",
        "outputId": "114f2eb4-d1a3-4084-ee5e-aba12102479c"
      },
      "outputs": [],
      "source": [
        "# plot emg features\n",
        "print(len(data.iloc[0]['features_EMG']))\n",
        "features = np.array([data.iloc[i].features_EMG[num_clips//2] for i in range(len(data))])\n",
        "reduced = TSNE().fit_transform(features)\n",
        "data['x'] = reduced[:, 0]\n",
        "data['y'] = reduced[:, 1]\n",
        "for i in range(12): # ek has 8 classes\n",
        "    filtered = data[data[\"label\"] == i]\n",
        "    # compute the central frame\n",
        "    plt.scatter(filtered['x'], filtered['y'], c=colors['AN'][i], label=labels['AN'][i])\n",
        "plt.legend()\n",
        "plt.title('EMG train features')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mxOr7-o7bQXP"
      },
      "source": [
        "# Train VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHA--nTubPuf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJwhsYyR_oD-"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "#VAE EMG train and save\n",
        "\n",
        "cd aml22-ego && git pull origin vae\n",
        "\n",
        "PYTHON_PATH=/usr/local/envs/egovision/bin/python\n",
        "\n",
        "$PYTHON_PATH train_VAE_features_EMG.py action=\"train_and_save\" \\\n",
        "  name=\"VAE_EMG_2 full-aug lr1e-3 wkld1 sum\" \\\n",
        "  config=configs/VAE_save_feat_EMG.yaml \\\n",
        "  dataset.shift=ActionNet-ActionNet \\\n",
        "  wandb_name='vae' \\\n",
        "  wandb_dir='Experiment_logs'  \\\n",
        "  dataset.RGB.data_path=../ek_data/frames \\\n",
        "  dataset.EMG.features_name='ACTIONNET_EMG/EMG_no-clip' \\\n",
        "  models.EMG.model='VAE' \\\n",
        "  models.EMG.lr=1e-3"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
