{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabbiurlaro/aml22-ego/blob/vae/augment_datset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW53_TEF0JuB"
      },
      "source": [
        "# INSTALL AND GIT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2GdURZvMjb9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9294266-78fa-4bf9-f247-343c79a63baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'aml22-ego'...\n",
            "remote: Enumerating objects: 3606, done.\u001b[K\n",
            "remote: Counting objects: 100% (258/258), done.\u001b[K\n",
            "remote: Compressing objects: 100% (129/129), done.\u001b[K\n",
            "remote: Total 3606 (delta 167), reused 206 (delta 128), pack-reused 3348\u001b[K\n",
            "Receiving objects: 100% (3606/3606), 1.51 GiB | 31.44 MiB/s, done.\n",
            "Resolving deltas: 100% (2633/2633), done.\n",
            "Updating files: 100% (45/45), done.\n",
            "Updating files: 100% (161/161), done.\n",
            "Branch 'vae' set up to track remote branch 'vae' from 'origin'.\n",
            "Switched to a new branch 'vae'\n"
          ]
        }
      ],
      "source": [
        "!rm -rf sample_data\n",
        "\n",
        "!git clone https://github.com/gabbiurlaro/aml22-ego.git aml22-ego\n",
        "!cd aml22-ego && git checkout vae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FhUB2tNujdxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce6bb28d-efcc-4799-ef01-45892f5255af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AuAI_gnfjeQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee8caeca-04cc-448b-bd87-a906f275f600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è¨ Downloading https://github.com/conda-forge/miniforge/releases/download/23.1.0-1/Mambaforge-23.1.0-1-Linux-x86_64.sh...\n",
            "üì¶ Installing...\n",
            "üìå Adjusting configuration...\n",
            "ü©π Patching environment...\n",
            "‚è≤ Done in 0:00:12\n",
            "üîÅ Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "# Install conda\n",
        "\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jItTAWYnuUO5"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /usr/local/envs/egovision\n",
        "!tar xf /content/drive/MyDrive/egovision.tar.gz --directory=/usr/local/envs/egovision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CHT3O2ka_M5"
      },
      "source": [
        "# Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pVgmYFLjPgt"
      },
      "outputs": [],
      "source": [
        "import pickle \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pywt\n",
        "import torch\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from scipy.interpolate import CubicSpline\n",
        "import random\n",
        "import torchvision.transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "train =  pd.DataFrame(pd.read_pickle('/content/aml22-ego/saved_features/ACTIONNET_EMG/EMG_no-clip_ActionNet_train.pkl'))\n",
        "test =  pd.DataFrame(pd.read_pickle('/content/aml22-ego/saved_features/ACTIONNET_EMG/EMG_no-clip_ActionNet_test.pkl'))\n",
        "original_train = pd.DataFrame(pd.read_pickle('/content/drive/MyDrive/train_val_EMG/ActionNet_train.pkl'))\n",
        "original_test = pd.DataFrame(pd.read_pickle('/content/drive/MyDrive/train_val_EMG/ActionNet_test.pkl'))\n",
        "original_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddtzSq60jPgw"
      },
      "outputs": [],
      "source": [
        "from utils.loaders import ActionNetDataset\n",
        "sigma = 0.1\n",
        "wavelet_name = 'db7' #Wavelet name (e.g., Daubechies 4)\n",
        "decomposition_level = 5 # # Number of decomposition levels\n",
        "detail_factor = 0 # Scaling factor for modifying detail coefficients\n",
        "  \n",
        "num_clips = 5\n",
        "batch_Size = 1\n",
        "\n",
        "\n",
        "train =  ActionNetDataset('ActionNet', ['EMG'], 'train', {'stride': 2, 'annotations_path':'/content/drive/MyDrive/train_val_EMG'}, {'EMG': 32}, 5, {'EMG': False},\n",
        "                                                                       None, load_feat=False, additional_info=True)\n",
        "#pd.DataFrame(pd.read_pickle('/content/aml22-ego/saved_features/ACTIONNET_EMG/EMG_no-clip_ActionNet_train.pkl'))\n",
        "test =  ActionNetDataset('ActionNet', ['EMG'], 'test', {'stride': 2, 'annotations_path':'/content/drive/MyDrive/train_val_EMG'}, {'EMG': 32}, 5, {'EMG': False},\n",
        "                                                                       None, load_feat=False,  additional_info=True)\n",
        "\n",
        "train.list_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "hw5qXKROjPgv"
      },
      "outputs": [],
      "source": [
        "def wavelet_decomposition(signal, wavelet_name, decomposition_level, detail_factor):\n",
        "    #print('WD :', signal.shape)\n",
        "    coeffs = pywt.wavedec(signal, wavelet=wavelet_name, level=decomposition_level)\n",
        "    cA, cD = coeffs[0], coeffs[1:]  # Approximation and detail coefficients\n",
        "    \n",
        "    # Modify detail coefficients\n",
        "    cD_modified = [detail_factor * cd for cd in cD]\n",
        "    \n",
        "    # Reconstruct the augmented signal\n",
        "    augmented_coeffs = [cA] + cD_modified\n",
        "    augmented_signal = torch.tensor(pywt.waverec(augmented_coeffs, wavelet=wavelet_name))\n",
        "    \n",
        "    return augmented_signal\n",
        "\n",
        "\n",
        "class WaveletDecompositionTransform:\n",
        "    def __init__(self, wavelet_name, decomposition_level, detail_factor, num_clips):\n",
        "        self.wavelet_name = wavelet_name\n",
        "        self.decomposition_level = decomposition_level\n",
        "        self.detail_factor = detail_factor\n",
        "        self.num_clips = num_clips\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        signals = sample['features_EMG']\n",
        "        augmented_signals = []\n",
        "        for i in range(self.num_clips):\n",
        "          \n",
        "          augmented_signals.append(wavelet_decomposition(signals[i], self.wavelet_name, self.decomposition_level, self.detail_factor))\n",
        "        # Create a new augmented sample\n",
        "        augmented_sample = {\n",
        "            'features_EMG': torch.stack(augmented_signals).reshape(5, 16, 32, 32),\n",
        "            'label': sample['label'],\n",
        "            'uid': sample['uid'],\n",
        "            'untrimmed_video_name': sample['untrimmed_video_name']\n",
        "        }\n",
        "        #print('WD',augmented_sample['features_EMG'].shape)\n",
        "        \n",
        "        return augmented_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "LGxv8xQ7jPgv"
      },
      "outputs": [],
      "source": [
        "def magnitude_warping(signal, variance=0.01):\n",
        "    T = signal.size(1)\n",
        "    #print('MW :', T)\n",
        "    t = torch.linspace(0, 1, T)  # Equidistant time points\n",
        "    r = torch.randn(T)  # Random numbers from a normal distribution\n",
        "    r = torch.clamp(r, -2.0, 2.0)  # Limit the range of random numbers to avoid extreme warping\n",
        "    \n",
        "    # Generate a smooth curve using cubic splines\n",
        "    spline = CubicSpline(t, r)\n",
        "    cubic_spline = torch.from_numpy(spline(t)).float()\n",
        "    \n",
        "    # Elementwise product of the interpolated curve with the signal\n",
        "    warped_signal = torch.Tensor(signal * (1.0 + variance * cubic_spline))\n",
        "\n",
        "    return warped_signal\n",
        "\n",
        "class MagnitudeWarpingTransform:\n",
        "    def __init__(self, variance, num_clips):\n",
        "        self.variance= variance\n",
        "        self.num_clips = num_clips\n",
        "    \n",
        "    def __call__(self, sample):\n",
        "        signals = sample['features_EMG']\n",
        "        augmented_signals = []\n",
        "        for i in range(self.num_clips):\n",
        "         # print('MW1: ', signals.shape)\n",
        "          augmented_signals.append(magnitude_warping(signals[i], variance=self.variance))\n",
        "        # Create a new augmented sample\n",
        "        augmented_sample = {\n",
        "            'features_EMG': torch.stack(augmented_signals),\n",
        "            'label': sample['label'],\n",
        "            'uid': sample['uid'],\n",
        "            'untrimmed_video_name': sample['untrimmed_video_name']\n",
        "        }\n",
        "        \n",
        "        return augmented_sample\n",
        "\n",
        "# Example usage\n",
        "#signal = torch.randn(1024)  # Assuming input signal of size 1024\n",
        "#warped_signal = magnitude_warping(signal, variance=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Fit_Dims:\n",
        "    def __init__(self, num_clips=5):\n",
        "        \n",
        "        self.num_clips = num_clips\n",
        "\n",
        "    def __call__(self, x):\n",
        "        signals = x[0]['EMG']\n",
        "        signals = signals.reshape(5,16, 32, 32)\n",
        "        #print(signals.shape)\n",
        "        augmented_sample = {\n",
        "              'features_EMG':signals,\n",
        "              'label': x[1],\n",
        "              'uid': x[3],\n",
        "              'untrimmed_video_name': x[2]\n",
        "          }\n",
        "        #print(augmented_sample['features_EMG'].shape)\n",
        "        return augmented_sample"
      ],
      "metadata": {
        "id": "mM4iZP3__zJr"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signal = train[0][0]['EMG']\n",
        "signal = signal[0]\n",
        "print('fd', type(signal), signal.shape)\n",
        "x = signal.reshape(5,-1,32)\n",
        "print('fd', type(x), x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH6YLd_lDX8T",
        "outputId": "ba35c050-33f3-4082-c32f-0742104c9171"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fd <class 'torch.Tensor'> torch.Size([160, 32])\n",
            "fd <class 'torch.Tensor'> torch.Size([5, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls\n",
        "for i in range(4):\n",
        "  filename = './aug_original/ActionNet_augmented_clips_' + ts[i]\n",
        "  with open(f\"{filename}_train.pkl\", \"wb\") as file:\n",
        "            pickle.dump(outs[0][i], file)\n",
        "  with open(f\"{filename}_test.pkl\", \"wb\") as file:\n",
        "             pickle.dump(outs[1][i], file)"
      ],
      "metadata": {
        "id": "_H0IVcLW1--Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e11a3cc-7ba2-4bd4-820b-fe614518eba7"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mACTIONNET\u001b[0m/  \u001b[01;34mACTIONNET_EMG\u001b[0m/  \u001b[01;34maug\u001b[0m/  \u001b[01;34maug_original\u001b[0m/  \u001b[01;34mEPIC\u001b[0m/  \u001b[01;34mreconstructed\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EODm1GEw1Y_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "mSjZDYKLBh_h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "76b40f76-4922-46ad-b301-d916b4d08554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/aml22-ego/saved_features/ACTIONNET_EMG_AUG/ (stored 0%)\n",
            "  adding: content/aml22-ego/saved_features/ACTIONNET_EMG_AUG/job_feature_extraction_test_train.pkl (deflated 11%)\n",
            "  adding: content/aml22-ego/saved_features/ACTIONNET_EMG_AUG/job_feature_extraction_train_train.pkl (deflated 11%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_839083ef-ae2d-402e-a1a8-620d024b1724\", \"feats_augs.zip\", 2209072)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "!zip -r /content/feats_augs.zip /content/aml22-ego/saved_features/ACTIONNET_EMG_AUG\n",
        "files.download('/content/feats_augs.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR-pPX-qbKtf"
      },
      "source": [
        "# Plot features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4Xev75IbKQG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZevU9cpo99zV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "num_clips = 5\n",
        "# EPIC-KITCHEN and ActionNet\n",
        "\n",
        "labels = {'EK':{\n",
        "        0 : \"take (get)\",\n",
        "        1 : \"put-down (put/place)\",\n",
        "        2 : \"open\",\n",
        "        3 : \"close\",\n",
        "        4 : \"wash (clean)\",\n",
        "        5 : \"cut\",\n",
        "        6 : \"stir (mix)\",\n",
        "        7 : \"pour\"\n",
        "}, 'AN': {\n",
        "        0 : \"Spread\",\n",
        "        1 : \"Get/Put\",\n",
        "        2 : \"Clear\",\n",
        "        3 : \"Slice\",\n",
        "        4 : \"Clean\",\n",
        "        5 : \"Pour\",\n",
        "        6 : \"Load\",\n",
        "        7 : \"Peel\",\n",
        "        8 : \"Open/Close\",\n",
        "        9 : \"Set\",\n",
        "        10 : \"Stack\",\n",
        "        11 : \"Unload\"\n",
        "\n",
        "}}\n",
        "\n",
        "colors = {'EK': {\n",
        "        0 : \"#A52A2A\",\n",
        "        1 : \"#DAA520\",\n",
        "        2 : \"#FF7F50\",\n",
        "        3 : \"#7BC8F6\",\n",
        "        4 : \"#FFFF14\",\n",
        "        5 : \"#76FF7B\",\n",
        "        6 : \"#13EAC9\",\n",
        "        7 : \"olive\"\n",
        "}, 'AN': {\n",
        "        0 : \"#A52A2A\",\n",
        "        1 : \"#DAA520\",\n",
        "        2 : \"#FF7F50\",\n",
        "        3 : \"#7BC8F6\",\n",
        "        4 : \"#FFFF14\",\n",
        "        5 : \"#76FF7B\",\n",
        "        6 : \"#13EAC9\",\n",
        "        7 : \"olive\",\n",
        "        8 : \"palegreen\",\n",
        "        9 : \"lightpink\",\n",
        "        10 : \"darkmagenta\",\n",
        "        11 : \"cadetblue\"\n",
        "}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i9t4C1m-EJi"
      },
      "outputs": [],
      "source": [
        "data_original = pd.DataFrame(pd.read_pickle('/content/aml22-ego/saved_features/ACTIONNET_EMG/EMG_no-clip_ActionNet_train.pkl')[\"features\"])\n",
        "data = pd.DataFrame(pd.read_pickle(\"/content/aml22-ego/saved_features/reconstructed/AUG_VAE_0.001_2023-05-24 16:15:44.696068_train.pkl\")[\"features\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olEjp7jS_0jQ"
      },
      "outputs": [],
      "source": [
        "data.iloc[0]\n",
        "ts = ['WD-MW', 'MW', 'WD', 'MW-WD']\n",
        "data_augmented_train = []\n",
        "for i in range(4):\n",
        "  data_augmented_train.append(pd.read_pickle('/content/aml22-ego/saved_features/aug/ActionNet_augmented_clips_'+ ts[i]+'_train.pkl'))\n",
        "data = pd.DataFrame(data_augmented_train[3]['features'] +data_augmented_train[2]['features']+\n",
        "                   data_augmented_train[1]['features'] +\n",
        "                    data_augmented_train[0]['features'] + data_original['features'])\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FWl8lnv-3PQ"
      },
      "outputs": [],
      "source": [
        "# plot emg features\n",
        "print(len(data.iloc[0]['features_EMG']))\n",
        "features = np.array([data.iloc[i].features_EMG[num_clips//2] for i in range(len(data))])\n",
        "reduced = TSNE().fit_transform(features)\n",
        "data['x'] = reduced[:, 0]\n",
        "data['y'] = reduced[:, 1]\n",
        "for i in range(12): # ek has 8 classes\n",
        "    filtered = data[data[\"label\"] == i]\n",
        "    # compute the central frame\n",
        "    plt.scatter(filtered['x'], filtered['y'], c=colors['AN'][i], label=labels['AN'][i])\n",
        "plt.legend()\n",
        "plt.title('EMG train features')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxOr7-o7bQXP"
      },
      "source": [
        "# Train VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHA--nTubPuf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJwhsYyR_oD-"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "#VAE EMG train and save\n",
        "\n",
        "cd aml22-ego && git pull origin vae\n",
        "\n",
        "PYTHON_PATH=/usr/local/envs/egovision/bin/python\n",
        "\n",
        "$PYTHON_PATH train_VAE_features_EMG.py action=\"train_and_save\" \\\n",
        "  name=\"VAE_EMG_2 full-aug lr1e-3 wkld1 sum\" \\\n",
        "  config=configs/VAE_save_feat_EMG.yaml \\\n",
        "  dataset.shift=ActionNet-ActionNet \\\n",
        "  wandb_name='vae' \\\n",
        "  wandb_dir='Experiment_logs'  \\\n",
        "  dataset.RGB.data_path=../ek_data/frames \\\n",
        "  dataset.EMG.features_name='ACTIONNET_EMG/EMG_no-clip' \\\n",
        "  models.EMG.model='VAE' \\\n",
        "  models.EMG.lr=1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Classifier"
      ],
      "metadata": {
        "id": "lz0363gSYozR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "ls\n",
        "cd aml22-ego/\n",
        "git stash\n",
        "git pull origin vae"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1avbQl8dKAq",
        "outputId": "40a8a207-b7fb-428b-d2ef-9208fdc5540c"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aml22-ego\n",
            "condacolab_install.log\n",
            "drive\n",
            "feats_augs.zip\n",
            "feats_def.zip\n",
            "No local changes to save\n",
            "Updating 85e3809..1d1dfd3\n",
            "Fast-forward\n",
            " configs/classifier_emg.yaml                        |   1 +\n",
            " saved_features/.DS_Store                           | Bin 6148 -> 6148 bytes\n",
            " .../job_feature_extraction_test_train.pkl          | Bin 0 -> 249144 bytes\n",
            " .../job_feature_extraction_train_train.pkl         | Bin 0 -> 2227571 bytes\n",
            " train_classifier_EMG.py                            |  66 ++++++++++++++++-----\n",
            " utils/loaders.py                                   |   4 +-\n",
            " 6 files changed, 53 insertions(+), 18 deletions(-)\n",
            " create mode 100644 saved_features/ACTIONNET_EMG_AUG/job_feature_extraction_test_train.pkl\n",
            " create mode 100644 saved_features/ACTIONNET_EMG_AUG/job_feature_extraction_train_train.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "From https://github.com/gabbiurlaro/aml22-ego\n",
            " * branch            vae        -> FETCH_HEAD\n",
            "   8a8750c..1d1dfd3  vae        -> origin/vae\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "#classifier EMG train, validate and save\n",
        "\n",
        "cd aml22-ego && git pull origin vae\n",
        "\n",
        "PYTHON_PATH=/usr/local/envs/egovision/bin/python\n",
        "\n",
        "$PYTHON_PATH train_classifier_EMG.py action=\"job_feature_extraction\" name=\"job_feature_extraction\" \\\n",
        "  config=configs/classifier_emg.yaml \\\n",
        "  dataset.shift=ActionNet-ActionNet \\\n",
        "  train.num_iter=600\\\n",
        "  wandb_name='vae'\\\n",
        "  wandb_dir='Experiment_logs'\\\n",
        "  dataset.RGB.data_path=../ek_data/frames  \\\n",
        "  models.EMG.model='EMG_classifier' \\\n",
        "  resume_from='./saved_models/EMG_classifier' \\\n",
        "  dataset.EMG.features_name='ACTIONNET_EMG/EMG_Normalized_no-clip' "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w804gCY2YoE8",
        "outputId": "b2cc78bf-c45e-43dd-b8e5-ce6102b54d51"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "From https://github.com/gabbiurlaro/aml22-ego\n",
            " * branch            vae        -> FETCH_HEAD\n",
            "2023-05-24 20:35:32 LOG INFO Running with parameters: \n",
            "  action: job_feature_extraction\n",
            "  name: job_feature_extraction\n",
            "  modality: ['EMG']\n",
            "  total_batch: 128\n",
            "  batch_size: 32\n",
            "  gpus: None\n",
            "  wandb_name: vae\n",
            "  resume_from: ./saved_models/EMG_classifier\n",
            "  logname: job_feature_extraction_ActionNet-ActionNet.log\n",
            "  models_dir: saved_models/job_feature_extraction/May24_20-35-31\n",
            "  train:\n",
            "    num_iter: 600\n",
            "    lr_steps: 500\n",
            "    eval_freq: 50\n",
            "    num_clips: 1\n",
            "    dense_sampling:\n",
            "      RGB: True\n",
            "      EMG: True\n",
            "    num_frames_per_clip:\n",
            "      RGB: 16\n",
            "      EMG: 32\n",
            "  save:\n",
            "    num_clips: 5\n",
            "    dense_sampling:\n",
            "      RGB: True\n",
            "      EMG: True\n",
            "    num_frames_per_clip:\n",
            "      RGB: 16\n",
            "      EMG: 32\n",
            "  test:\n",
            "    num_clips: 5\n",
            "    dense_sampling:\n",
            "      RGB: True\n",
            "      EMG: True\n",
            "    num_frames_per_clip:\n",
            "      RGB: 16\n",
            "      EMG: 32\n",
            "  dataset:\n",
            "    annotations_path: ../drive/MyDrive/train_val_EMG\n",
            "    shift: ActionNet-ActionNet\n",
            "    workers: 4\n",
            "    stride: 2\n",
            "    resolution: 224\n",
            "    RGB:\n",
            "      data_path: ../ek_data/frames\n",
            "      tmpl: img_{:010d}.jpg\n",
            "      features_name: test_feat_kinetics\n",
            "    Event:\n",
            "      rgb4e: 6\n",
            "    EMG:\n",
            "      features_name: ACTIONNET_EMG/EMG_Normalized_no-clip\n",
            "  shift: ActionNet-ActionNet\n",
            "  split: train\n",
            "  num_clips: 5\n",
            "  augmentation: True\n",
            "  models:\n",
            "    EMG:\n",
            "      model: EMG_classifier\n",
            "      dropout: 0.2\n",
            "      normalize: False\n",
            "      resolution: 224\n",
            "      kwargs:\n",
            "      lr_steps: 30\n",
            "      epochs: 100\n",
            "      lr: 0.01\n",
            "      sgd_momentum: 0.9\n",
            "      weight_decay: 0.005\n",
            "  config: configs/classifier_emg.yaml\n",
            "  wandb_dir: Experiment_logs\n",
            "  experiment_dir: job_feature_extraction/May24_20-35-31\n",
            "  log_dir: TEST_RESULTS/job_feature_extraction\n",
            "  logfile: TEST_RESULTS/job_feature_extraction/job_feature_extraction_ActionNet-ActionNet.log\n",
            "wandb: Currently logged in as: salvatoreadalberto-esposito (egovision-aml22). Use `wandb login --relogin` to force relogin\n",
            "wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "wandb: wandb version 0.15.3 is available!  To upgrade, please run:\n",
            "wandb:  $ pip install wandb --upgrade\n",
            "wandb: Tracking run with wandb version 0.13.4\n",
            "wandb: Run data is saved locally in /content/aml22-ego/wandb/run-20230524_203534-ryofi769\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run resilient-darkness-156\n",
            "wandb: ‚≠êÔ∏è View project at https://wandb.ai/egovision-aml22/aml22-ego\n",
            "wandb: üöÄ View run at https://wandb.ai/egovision-aml22/aml22-ego/runs/ryofi769\n",
            "2023-05-24 20:35:35 LOG INFO Instantiating models per modality\n",
            "2023-05-24 20:35:35 LOG INFO EMG_classifier Net\tModality: EMG\n",
            "2023-05-24 20:35:36 LOG INFO Loading model from ./saved_models/EMG_classifier\n",
            "2023-05-24 20:35:36 LOG INFO Restoring action-classifier for modality EMG from saved_models/EMG_classifier/May21_15-30-10/action-classifier_EMG_9.pth\n",
            "2023-05-24 20:35:36 LOG INFO EMG-Model for action-classifier restored at iter 540.0\n",
            "Best accuracy on val: 81.36 at iter 410.0\n",
            "Last accuracy on val: 79.66\n",
            "Last loss: 0.00\n",
            "2023-05-24 20:35:36 LOG INFO modalities: ['EMG']\n",
            "2023-05-24 20:35:36 LOG INFO  aug: True\n",
            "2023-05-24 20:35:37 LOG INFO Dataloader for ActionNet-train with 526 samples generated\n",
            "2023-05-24 20:35:37 LOG INFO jeez : saved_features/ACTIONNET_EMG/EMG_Normalized_no-clip_ActionNet_train.pkl\n",
            "/usr/local/envs/egovision/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2023-05-24 20:35:37 LOG INFO Dataloader for ActionNet-test with 59 samples generated\n",
            "2023-05-24 20:35:37 LOG INFO jeez : saved_features/ACTIONNET_EMG/EMG_Normalized_no-clip_ActionNet_test.pkl\n",
            "/usr/local/envs/egovision/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2023-05-24 20:35:37 LOG INFO Dataloader for ActionNet-train with 526 samples generated\n",
            "2023-05-24 20:35:37 LOG INFO jeez : saved_features/ACTIONNET_EMG/EMG_Normalized_no-clip_ActionNet_train.pkl\n",
            "/usr/local/envs/egovision/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2023-05-24 20:35:37 LOG INFO Dataloader for ActionNet-test with 59 samples generated\n",
            "2023-05-24 20:35:37 LOG INFO jeez : saved_features/ACTIONNET_EMG/EMG_Normalized_no-clip_ActionNet_test.pkl\n",
            "/usr/local/envs/egovision/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2023-05-24 20:35:38 LOG INFO Dataloader for ActionNet-train with 526 samples generated\n",
            "2023-05-24 20:35:38 LOG INFO jeez : saved_features/ACTIONNET_EMG/EMG_Normalized_no-clip_ActionNet_train.pkl\n",
            "/usr/local/envs/egovision/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2023-05-24 20:35:38 LOG INFO Dataloader for ActionNet-test with 59 samples generated\n",
            "2023-05-24 20:35:38 LOG INFO jeez : saved_features/ACTIONNET_EMG/EMG_Normalized_no-clip_ActionNet_test.pkl\n",
            "/usr/local/envs/egovision/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2023-05-24 20:35:38 LOG INFO Dataloader for ActionNet-train with 526 samples generated\n",
            "2023-05-24 20:35:38 LOG INFO jeez : saved_features/ACTIONNET_EMG/EMG_Normalized_no-clip_ActionNet_train.pkl\n",
            "/usr/local/envs/egovision/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2023-05-24 20:35:38 LOG INFO Dataloader for ActionNet-test with 59 samples generated\n",
            "2023-05-24 20:35:38 LOG INFO jeez : saved_features/ACTIONNET_EMG/EMG_Normalized_no-clip_ActionNet_test.pkl\n",
            "/usr/local/envs/egovision/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2023-05-24 20:35:38 LOG INFO Dataloader for ActionNet-train with 526 samples generated\n",
            "2023-05-24 20:35:39 LOG INFO jeez : saved_features/ACTIONNET_EMG/EMG_Normalized_no-clip_ActionNet_train.pkl\n",
            "/usr/local/envs/egovision/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2023-05-24 20:35:39 LOG INFO Dataloader for ActionNet-test with 59 samples generated\n",
            "2023-05-24 20:35:39 LOG INFO jeez : saved_features/ACTIONNET_EMG/EMG_Normalized_no-clip_ActionNet_test.pkl\n",
            "/usr/local/envs/egovision/lib/python3.7/site-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "2023-05-24 20:35:39 LOG INFO here\n",
            "Traceback (most recent call last):\n",
            "  File \"train_classifier_EMG.py\", line 461, in <module>\n",
            "    main()\n",
            "  File \"train_classifier_EMG.py\", line 151, in main\n",
            "    save_feat(action_classifier, train_loader[a], device, action_classifier.current_iter, num_classes, train=True)\n",
            "UnboundLocalError: local variable 'train_loader' referenced before assignment\n",
            "2023-05-24 20:35:39 LOG ERROR Uncaught exception\n",
            "Traceback (most recent call last):\n",
            "  File \"train_classifier_EMG.py\", line 461, in <module>\n",
            "    main()\n",
            "  File \"train_classifier_EMG.py\", line 151, in main\n",
            "    save_feat(action_classifier, train_loader[a], device, action_classifier.current_iter, num_classes, train=True)\n",
            "UnboundLocalError: local variable 'train_loader' referenced before assignment\n",
            "wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.\n",
            "wandb: - 0.001 MB of 0.009 MB uploaded (0.000 MB deduped)\rwandb: \\ 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)\rwandb: Synced resilient-darkness-156: https://wandb.ai/egovision-aml22/aml22-ego/runs/ryofi769\n",
            "wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "wandb: Find logs at: ./wandb/run-20230524_203534-ryofi769/logs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-188-604446bf63bd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#classifier EMG train, validate and save\\n\\ncd aml22-ego && git pull origin vae\\n\\nPYTHON_PATH=/usr/local/envs/egovision/bin/python\\n\\n$PYTHON_PATH train_classifier_EMG.py action=\"job_feature_extraction\" name=\"job_feature_extraction\" \\\\\\n  config=configs/classifier_emg.yaml \\\\\\n  dataset.shift=ActionNet-ActionNet \\\\\\n  train.num_iter=600\\\\\\n  wandb_name=\\'vae\\'\\\\\\n  wandb_dir=\\'Experiment_logs\\'\\\\\\n  dataset.RGB.data_path=../ek_data/frames  \\\\\\n  models.EMG.model=\\'EMG_classifier\\' \\\\\\n  resume_from=\\'./saved_models/EMG_classifier\\' \\\\\\n  dataset.EMG.features_name=\\'ACTIONNET_EMG/EMG_Normalized_no-clip\\' \\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'#classifier EMG train, validate and save\\n\\ncd aml22-ego && git pull origin vae\\n\\nPYTHON_PATH=/usr/local/envs/egovision/bin/python\\n\\n$PYTHON_PATH train_classifier_EMG.py action=\"job_feature_extraction\" name=\"job_feature_extraction\" \\\\\\n  config=configs/classifier_emg.yaml \\\\\\n  dataset.shift=ActionNet-ActionNet \\\\\\n  train.num_iter=600\\\\\\n  wandb_name=\\'vae\\'\\\\\\n  wandb_dir=\\'Experiment_logs\\'\\\\\\n  dataset.RGB.data_path=../ek_data/frames  \\\\\\n  models.EMG.model=\\'EMG_classifier\\' \\\\\\n  resume_from=\\'./saved_models/EMG_classifier\\' \\\\\\n  dataset.EMG.features_name=\\'ACTIONNET_EMG/EMG_Normalized_no-clip\\' \\n'' returned non-zero exit status 1."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}