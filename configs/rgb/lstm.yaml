action: train # train or test
name: ???
modality: ["RGB"]
shift: D1-D1
split: train
resume_from: ???
num_clips: 10
wandb_name: '(RGB)'
in_features: 1024
total_batch: 128 # total batch size if training is done with gradient accumulation
batch_size: 32 # batch size for the forward
gpus: null # gpus adopted
logname: null # name of the logs
models_dir: null # directory containing all the models
wandb_dir: Experiment_logs # directory for the wandb logs

augmentation: False

train:
  num_iter: 500        # number of training iterations with total_batch size
  eval_freq: 50        # evaluation frequency
  num_clips: 10        # clips adopted in training
  embedding_size: 1024 # size of the embedding vector
  dense_sampling:      # sampling version adopted in training for each modality
    RGB: True
    EMG: True
  num_frames_per_clip: # number of frames adopted in training for each modality
    RGB: 16
    EMG: 32

save: 
  num_clips: 10        # clips adopted in training
  dense_sampling:      # sampling version adopted in training for each modality
    RGB: True
    EMG: True
  num_frames_per_clip: # number of frames adopted in training for each modality
    RGB: 16
    EMG: 32

test:
  num_clips: 10        # number of clips in testing
  dense_sampling:      # sampling version adopted in test for each modality
    RGB: True
    EMG: True
  num_frames_per_clip: # number of frames adopted in test for each modality
    RGB: 16
    EMG: 32

dataset:
  annotations_path: 'train_val' # path for the annotations data
  shift: D1-D1  # shifts of the dataset
  workers: 4                  # number of workers for the dataloader
  stride: 1                   # stride in case of dense sampling
  RGB:
    data_path: '../ek_data/frames'
    features_name: 'EPIC/new/FT_16f_10c_D'

models:
  RGB:
    model: ActionLSTM  # model name
    dropout: 0.6                     # dropout adopted   
    normalize: False                  # normalization adopted         
    transform: False                  # transformation adopted        
    lr: 0.01                         # learning rate  
    lr_steps: 200                      # steps before reducing learning rate
    sgd_momentum: 0.9                 # momentum for the optimizer
    weight_decay: 0.001     