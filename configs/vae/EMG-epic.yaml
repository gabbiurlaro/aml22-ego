action: "train_and_save" # train or test
name: ??? # name of the experiment needed for the logs
modality: ["RGB"] # modality used

total_batch: 128 # total batch size if training is done with gradient accumulation
batch_size: 32 # batch size for the forward

gpus: null # gpus adopted
wandb_name: "RGB-sEMG" # needed for wandb logging
resume_from: null # checkpoint directory
logname: null # name of the logs
models_dir: null # directory containing all the models
split: null

last_model: 
  encoder: "saved_models/RGB_sEMG/2023-06-18/RGB_sEMG_lr0.0001_b1e-05_2023-06-18 10:19:40.246353.pth"
  decoder: "saved_models/RGB_sEMG/2023-06-18/RGB_sEMG_lr0.0001_b1e-05_2023-06-18 10:19:40.246353.pth"

train:
  num_iter: 200 # number of training iterations with total_batch size
  lr_steps: 50 # steps before reducing learning rate
  eval_freq: 50 # evaluation frequency
  num_clips: 10 # clips adopted in training
  in_feature_size: 1024
  out_feature_size: 1024
  bottleneck_size: 256

save: 
  num_clips: 10 # clips adopted in save

test:
  num_clips: 10 # number of clips in testing


dataset:
  annotations_path: train_val
  shift: S04-S04
  workers: 4
  stride: 2
  resolution: 224
  RGB:
    data_path: "../ek_data/frames"
    tmpl: "img_{:010d}.jpg"
    features_name: EPIC/FT_D_D1_16f_10c
models:
  vae:
    architecture:
      bottleneck: 256
    model: VAE
    dropout: 0.2
    kwargs: {}
    epochs: 300
    lr_steps: 200
    lr: 0.01
    lr_gamma: 0.01
    beta: 0.0001

