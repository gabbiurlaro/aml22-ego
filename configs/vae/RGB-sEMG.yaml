action: "train_and_save" # train or test
name: ??? # name of the experiment needed for the logs
modality: ["RGB", "EMG"] # modality used

total_batch: 128 # total batch size if training is done with gradient accumulation
batch_size: 32 # batch size for the forward

gpus: null # gpus adopted
wandb_name: "RGB-sEMG" # needed for wandb logging
resume_from: null # checkpoint directory
logname: null # name of the logs
models_dir: null # directory containing all the models
split: null

last_model: 
  encoder: "saved_models/VAE_RGB/2023-06-10/VAE_RGB_lr0.001.pth"
  decoder: "saved_models/VAE_EMG/VAE_EMG_lr0.0001_b1e-05_2023-06-10 11:40:54.140793.pth"

train:
  num_iter: 300 # number of training iterations with total_batch size
  lr_steps: 50 # steps before reducing learning rate
  eval_freq: 50 # evaluation frequency
  num_clips: 5 # clips adopted in training
  in_feature_size: 1024
  out_feature_size: 1024
  bottleneck_size: 256

save: 
  num_clips: 10 # clips adopted in save

test:
  num_clips: 10 # number of clips in testing


dataset:
  annotations_path: train_val
  shift: S04-S04
  workers: 4
  stride: 2
  resolution: 224
  RGB:
    data_path: "../ek_data/frames"
    tmpl: "img_{:010d}.jpg"
    features_name: ACTIONNET/FT_D_16f_10c
  EMG:
    features_name: "ACTIONNET_EMG/EMG_nf-32_clip-10_embedding_size-1024_U"
models:
  vae:
    architecture:
      bottleneck: 256
    model: VAE
    dropout: 0.2
    kwargs: {}
    epochs: 200
    lr_steps: 50
    lr: 0.0001
    lr_gamma: 0.01
    beta: 1e-5


